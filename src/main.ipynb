{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center; font-size: 30px; color: navy; font-weight: bold;\">\n",
    "    Assessment - Coleta de Dados com Python via APIs e WebScraping <br>\n",
    "    Maik Júnior dos Santos\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1: Docs e Regex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 1:\n",
    "Baixe seu perfil no Linkedin em PDF e utilize o PyPDF2 para construir uma função que retorne a string do texto completo do documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==> Iportando biblioteca\n",
    "from PyPDF2 import PdfReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==> lendo .pdf\n",
    "pdf = PdfReader('../file/Profile.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==> Funcao extrai dados do arquivo pdf\n",
    "def le_pdf(documento):\n",
    "    text = '\\n\\n'.join([x.extract_text() for x in documento.pages])\n",
    "    return text\n",
    "\n",
    "#==> Chamando funcao\n",
    "texto = le_pdf(pdf)\n",
    "texto = ' '.join(texto.split())\n",
    "texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 2:\n",
    "Utilize Regex (módulo `re` nativo do Python) para criar uma função que, a partir do texto extraído, retorne um dicionário com as seguintes informações: \n",
    "- Seu número de telefone;\n",
    "- Seu endereço de email; \n",
    "- O link do seu perfil no Linkedin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==> Importando biblioteca\n",
    "import re\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==> Recuperando nome\n",
    "nome1 = re.search(r'maik', texto)\n",
    "nome1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#== > Recuperando numero de telefone\n",
    "telefone1 = re.search(r'9\\d{4}-?\\d{4}', texto)\n",
    "telefone1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#== > Recuperando email\n",
    "email1 = re.search(r'\\s*[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.\\s*[a-zA-Z]{2,}\\s*', texto)\n",
    "email1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#== > Recuperando link perfil\n",
    "perfil1 = re.search(r'www\\.linkedin\\.com\\/in\\/[a-zA-Z0-9-]+', texto)\n",
    "perfil1[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 3:\n",
    "Aplique as funções geradas nas questões 1 e 2 para fazer o mesmo com o PDF em anexo (perfil do professor) e crie um CSV com as informações extraídas (colunas: nome, telefone, email e perfil) utilizando o módulo `csv` nativo do Python. Obs.: ao final os padrões utilizados no Regex devem abarcar os conteúdos dos dois PDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==> lendo .pdf\n",
    "pdf2 = PdfReader('../file/ViniciusBS_Perfil_2024_08_12.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#==> Chamando funcao\n",
    "texto2 = le_pdf(pdf2)\n",
    "texto2 = ' '.join(texto2.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==> Recuperando informacoes\n",
    "nome1 = re.search(r'maik', texto)[0]\n",
    "telefone1 = re.search(r'9\\d{4}-?\\d{4}', texto)[0]\n",
    "email1 = re.search(r'\\s*[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.\\s*[a-zA-Z]{2,}\\s*', texto)[0]\n",
    "perfil1 = re.search(r'www\\.linkedin\\.com\\/in\\/[a-zA-Z0-9-]+', texto)[0]\n",
    "\n",
    "nome2 = re.search(r'vinicius', texto2)[0]\n",
    "telefone2 = re.search(r'9\\d{4}-?\\d{4}', texto2)[0]\n",
    "email2 = re.search(r'\\s*[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.\\s*[a-zA-Z]{2,}\\s*', texto2)[0]\n",
    "perfil2 = re.search(r'www\\.linkedin\\.com\\/in\\/[a-zA-Z0-9-]+', texto2)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==> Dicionario\n",
    "dados = {'Nome': [nome1,nome2],'Telefone': [telefone1,telefone2],'Email': [email1,email2],'Linkedin': [perfil1,perfil2]}\n",
    "\n",
    "#==> Escrevendo .csv\n",
    "with open('../file/perfis.csv', mode='w', newline='', encoding='utf-8') as arquivo_csv:\n",
    "    escritor = csv.DictWriter(arquivo_csv, fieldnames=dados.keys()) \n",
    "    escritor.writeheader()\n",
    "    for linha in zip(*dados.values()):\n",
    "        escritor.writerow(dict(zip(dados.keys(), linha)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 2: APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 4:\n",
    "Explore o “playground” da API do SimilarWeb encontrada no RapidAPI ([link](https://rapidapi.com/Glavier/api/similarweb12/playground/)) e inscreva-se no plano gratuito. Então, crie um código para obter os dados dos 10 primeiros sites listados em “top-websites”, salvando-os em um dataframe do Pandas e, por fim, em um arquivo CSV usando o próprio Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==> Importando bibliotecas\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==> Requisicao\n",
    "url = \"https://similarweb12.p.rapidapi.com/v3/top-websites/\"\n",
    "\n",
    "headers = {\n",
    "\t\"x-rapidapi-key\": \"471fd59fe1mshb443388ad14bfc4p1c1b01jsn763e6bb76f1d\",\n",
    "\t\"x-rapidapi-host\": \"similarweb12.p.rapidapi.com\"\n",
    "}\n",
    "\n",
    "resposta = requests.get(url, headers=headers)\n",
    "\n",
    "#==> Resultado dataframe\n",
    "df = pd.DataFrame(resposta.json()['sites'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==> Criando .csv\n",
    "df[:10].to_csv('../file/top_sites.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==> Lendo .csv\n",
    "pd.read_csv('../file/top_sites.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 3: XPath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 5:\n",
    "Utilize o arquivo XML em anexo e a biblioteca `lxml` com caminhos relativos de XPath para:\n",
    "- Selecionar os nomes de todos *estudantes* que estejam no 2º ano ou acima dele;\n",
    "- Selecionar o nome do *professor* de Estruturas de Dados (course: \"Data Structures\");\n",
    "- Selecionar os títulos de todos os *cursos* ofertados pelo departamento de Ciência da Computação (department: Computer Science);\n",
    "- Selecionar os nomes de todos os *departamentos* que sejam pertencentes à Escola de Engenharia (college: Engineering)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==> Importando biblioteca\n",
    "from lxml import etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==> Lendo .xml\n",
    "root = etree.parse('../file/AT.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==> 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root.xpath(\"//student[@year > 2]/text()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==> 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root.xpath(\"//title/text()\")\n",
    "\n",
    "# root.xpath(\"//professor/text()\")\n",
    "\n",
    "root.xpath(\"//course[title='Data Structures']/professor/text()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==> 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root.xpath(\"//department[@name='Computer Science']//title/text()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==> 4\n",
    "root.xpath(\"//college[@name='Engineering']/department/@name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 4: CSS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 6:\n",
    "Utilize o arquivo XML em anexo e a biblioteca `lxml` com seletores de CSS para:\n",
    "- Selecionar os títulos de todos os cursos cujos professores possuem estabilidade (tenure);\n",
    "- Selecionar os títulos de todos os cursos que possuem horário de início pela manhã (AM). **Dica:** cuidado com nomes antigos de pseudo-classes; caso algum não funcione, tente o nome antigo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==> Importando biblioteca\n",
    "from lxml import html\n",
    "from lxml import etree\n",
    "from lxml.cssselect import CSSSelector as sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==> 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==> Lendo .xml\n",
    "tree = etree.parse('../file/AT.xml')\n",
    "\n",
    "#==> Filtro\n",
    "selecao = sel('professor[tenure=\"true\"]')\n",
    "\n",
    "#==> Recuperando nomes\n",
    "[v.text for v in selecao(tree)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==> 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==> Lendo .xml\n",
    "tree = etree.parse('../file/AT.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==> Selecionando cursos\n",
    "selector = sel('course')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==> Seletor\n",
    "cursos = selector(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==> Recuperando cursos\n",
    "[curso.cssselect('title')[0].text for curso in cursos if curso.cssselect('schedule time') and \"AM\" in curso.cssselect('schedule time')[0].text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 5: WebCrawling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 7:\n",
    "Examine um site de sua escolha na lista de sites fornecida em anexo e descubra o padrão de URL para paginação que ele aceita. Então, utilize-o para obter uma lista de links de notícias requisitando as 2 primeiras páginas e raspando os links de cada uma através de um único seletor de CSS aplicado via `BeautifulSoup`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==> Importando biblioteca\n",
    "import requests\n",
    "import random\n",
    "import time\n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36'}\n",
    "\n",
    "#==> Recuperando dados unicos\n",
    "def uniques(seq: list | tuple) -> list | tuple:\n",
    "    out = []\n",
    "    for x in seq:\n",
    "        if x not in out:\n",
    "            out.append(x)\n",
    "    return out if isinstance(seq, list) else tuple(out)\n",
    "\n",
    "#==> Fazendo requisicao\n",
    "def requests_page_interval(url: str | bytes) -> str:\n",
    "    time.sleep(random.randint(1,2))\n",
    "    resposta = requests.get(url, headers=header, timeout=30)\n",
    "    return resposta.text\n",
    "\n",
    "dominio = 'https://diariodoestadomt.com.br/'\n",
    "\n",
    "#==> Recuperando links\n",
    "def web_crawler(url_pattern: str, selector: str, range_config: tuple) -> list:\n",
    "    links = []\n",
    "    for num in range(*range_config):\n",
    "        pagina = requests_page_interval(url_pattern.format(num))\n",
    "        soup = bs(pagina, 'lxml')\n",
    "        urls = [tag.get('href') for tag in soup.select(selector)]\n",
    "        urls_completas = [link if link.startswith('http') else dominio + link for link in urls]\n",
    "        links.extend(urls_completas)\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = web_crawler('https://diariodoestadomt.com.br/noticias/{}', 'a.news', (0, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==> Links das paginas\n",
    "links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 6: WebScraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 8:\n",
    "Faça um loop para os 3 primeiros links da lista obtida na questão anterior requisitando o HTML de cada página com a biblioteca que preferir (`urllib`, `requests`, etc.) e aplicando funções baseadas em `BeautifulSoup` para capturar e por fim salvar em um mesmo arquivo JSON, junto à URL de cada notícia e ao datetime do momento da requisição de cada página:\n",
    "- O objeto datetime (timezone-aware) da data e hora da publicação da notícia;\n",
    "- O título da notícia;\n",
    "- O corpo do texto da notícia;\n",
    "- O subtítulo da notícia (se houver);\n",
    "- O autor ou autores da notícia (se houver)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==> Importando bibliotecas\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import random\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==> Info cliente\n",
    "headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==> Lista\n",
    "webscraping = []\n",
    "\n",
    "#==> Iterando em links para recuperar dados\n",
    "for i, v in enumerate(links[1:4]):\n",
    "    resposta = requests.get(v, headers=headers, timeout=120)\n",
    "    time_requisicao = resposta.headers.get('Date')\n",
    "\n",
    "    #==> Gravando resposta\n",
    "    with open(f'../file/{v[-9:]}.html', 'w', encoding='utf-8') as f:\n",
    "        f.write(resposta.text)\n",
    "    \n",
    "    #==> Lendo .html\n",
    "    with open(f'../file/{v[-9:]}.html', 'r', encoding='utf-8') as r: \n",
    "        noticia = r.read()\n",
    "\n",
    "    #==> Rcuperando .html\n",
    "    soup = bs(noticia, 'html.parser')\n",
    "\n",
    "    noticias = {\n",
    "        'url': v,\n",
    "        'requisicao_datetime': time_requisicao,\n",
    "        'timezone-aware': soup.find('h2').find_next_sibling('p').find_next_sibling('p').text.strip(),\n",
    "        'noticia_titulo': soup.find('title').text.strip(),\n",
    "        'noticia_corpo': ''.join([p.get_text().strip().replace('\\n', '') for p in soup.find_all('p')][4:-10]),\n",
    "        'noticia_subtitulo': 'NA',\n",
    "        'noticia_autor': ''.join([p.get_text().strip().replace('\\n', '') for p in soup.find_all('p')][-10]),\n",
    "    }\n",
    "\n",
    "    #==> Adicionando a lista\n",
    "    webscraping.append(noticias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==> Criando .json\n",
    "with open('../file/noticias.json', 'w', encoding='utf-8') as arquivo:\n",
    "    json.dump(webscraping, arquivo, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==> Criando .json\n",
    "with open('../file/noticias.json', 'r', encoding='utf-8') as r:\n",
    "    noticias_json = json.load(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noticias_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 7: Scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 9:\n",
    "Escolha um dos sites da lista fornecida (que não tenha sido escolhido nas anteriores) para montar um projeto no Scrapy que abarque tanto o Crawling quanto o Scraping, a fim de rodá-lo tal como na questão anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==> Importando biblioteca\n",
    "import scrapy\n",
    "import json\n",
    "import pprint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==> Criando projeto scrapping \n",
    "# !scrapy startproject at_scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==> Executando comando no bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# cd /home/maik/_repos/ATDR2/src/at_scrapy/at_scrapy/spiders\n",
    "# scrapy genspider rondonia_noticias rondoniadinamica.com/ultimas-noticias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==> Executando comando no bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# cd /home/maik/_repos/ATDR2/src/at_scrapy/at_scrapy/spiders\n",
    "# scrapy crawl rondonia_noticias -o rondonia_noticias.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==> Executando comando no bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# cd /home/maik/_repos/ATDR2/src/at_scrapy/at_scrapy/spiders\n",
    "# scrapy crawl rondonia_noticias -o rondonia_noticias.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==> Lendo .json\n",
    "with open('/home/maik/_repos/ATDR2/src/at_scrapy/at_scrapy/spiders/rondonia_noticias.json', 'r', encoding='utf-8') as f:\n",
    "    dados = json.load(f) \n",
    "    \n",
    "dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 8: Selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 10:\n",
    "Extraia uma lista de empregos do site [**Indeed**](https://br.indeed.com). Extraia os títulos dos empregos da primeira página de resultados ao pesquisar por \"Data Scientist\" na área da capital de seu estado. O site usa JavaScript para carregar as listas dinamicamente, o que significa que você não pode recuperar esses dados simplesmente usando solicitações ou `BeautifulSoup`. Escreva um script em Python usando Selenium para extrair os títulos dos empregos desta página junto a outras informações que você considere relevante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==> Importando biblioteca\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==> Instanciando\n",
    "driver = webdriver.Firefox() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==> Acessando site\n",
    "driver.get('https://br.indeed.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==> Identificando elementos\n",
    "keyword_input = driver.find_element(By.CSS_SELECTOR, '#text-input-what')\n",
    "region_input = driver.find_element(By.CSS_SELECTOR, '#text-input-where')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==> Limpando campos\n",
    "keyword_input.clear()\n",
    "region_input.clear()\n",
    "\n",
    "#==> Enviando dados\n",
    "keyword_input.send_keys('Data Scientist')\n",
    "region_input.send_keys('Parana')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==> Idendificando/clicando no botao\n",
    "submit_button = driver.find_element(By.CSS_SELECTOR, 'button[type=submit]')\n",
    "submit_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==> Recuperando vagas\n",
    "cards = driver.find_elements(By.CSS_SELECTOR, 'div.job_seen_beacon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==> Iterando nas vagas\n",
    "for card in cards:\n",
    "    title = card.find_element(By.CSS_SELECTOR, 'h2 > a > span').text\n",
    "    company = card.find_element(By.CSS_SELECTOR, '[data-testid=\"company-name\"]').text\n",
    "    presentation_div = driver.find_element(By.CSS_SELECTOR, 'div[role=\"presentation\"].css-1u8dvic').text\n",
    "    print(title, company, presentation_div,  sep=' | ', end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
